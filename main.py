from mcp.server.fastmcp import FastMCP
import logging
from llama_index.core import VectorStoreIndex, Document, StorageContext, Settings
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.retrievers import VectorIndexRetriever
from environments.config import setup_chroma
from posts.notion import fetch_notion_pages

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastMCP ì„œë²„
mcp = FastMCP("content-search-server")
# ChromaDB ì„¤ì •
chroma_collection = setup_chroma()

# LlamaIndex ì„¤ì •
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

Settings.cache_dir = ".llama_cache"

# ê¸€ë¡œë²Œ ì¸ë±ìŠ¤ ë³€ìˆ˜
index = None

@mcp.tool()
async def index_all_content() -> str:
    """
    ëª¨ë“  í”Œë«í¼(Notion, GitHub, Tistory ë“±)ì˜ ê¸€ì„ ìˆ˜ì§‘í•˜ê³  ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    ğŸ”¹ ìµœì´ˆ ì‹¤í–‰ ë˜ëŠ” í•„ìš”í•œ ê²½ìš° ì‹¤í–‰í•©ë‹ˆë‹¤.
    ğŸ”¹ LlamaIndex/Chromaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ìë™ìœ¼ë¡œ ì²­í‚¹í•˜ê³  ì„ë² ë”©í•©ë‹ˆë‹¤.
    ğŸ”¹ ì´í›„ search_content ë„êµ¬ë¡œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

    Returns:
        str: ì¸ë±ì‹± ê²°ê³¼ ìš”ì•½
            - ìˆ˜ì§‘ëœ ë¬¸ì„œ ìˆ˜
            - ìƒì„±ëœ ì²­í¬ ìˆ˜
            - ì„±ê³µ/ì‹¤íŒ¨ ë©”ì‹œì§€
    """
    global index
    
    try:
        logger.info("ğŸ“¥ ë¬¸ì„œ ìˆ˜ì§‘ ì¤‘...")
        
        # ë¬¸ì„œ ìˆ˜ì§‘
        notion_docs = await fetch_notion_pages()
        # github_docs = await fetch_github_files()
        # tistory_docs = await fetch_tistory_posts()
        
        if not notion_docs:
            return "âŒ ìˆ˜ì§‘ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
        
        logger.info(f"ğŸ“Š ì´ {len(notion_docs)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ì—ì„œ doc_id ê°€ì ¸ì˜¤ê¸°
        existing_ids = [m['id'] for m in chroma_collection.get()['ids']]

        # ì‹ ê·œ ë¬¸ì„œë§Œ í•„í„°ë§
        new_documents = [
            Document(
                text=doc['content'],
                metadata={
                    'title': doc['title'],
                    'platform': doc['platform'],
                    'url': doc['url'],
                    'date': doc.get('date', ''),
                    'doc_id': doc['id']
                }
            )
            for doc in notion_docs
            if doc['id'] not in existing_ids
        ]

        if not new_documents:
            return "ì‹ ê·œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤."

        logger.info(f"{len(new_documents)}ê°œì˜ ì‹ ê·œ ë¬¸ì„œ ì¸ë±ì‹± ì¤‘...")

        index =  VectorStoreIndex.from_documents(
            new_documents,
            storage_context=storage_context,
            show_progress=True
        )
        
        return f"""
            âœ… ì¸ë±ì‹± ì™„ë£Œ!

            ğŸ“Š ìˆ˜ì§‘ëœ ë¬¸ì„œ: {len(notion_docs)}ê°œ
            âœ‚ï¸ LlamaIndexê°€ ìë™ìœ¼ë¡œ ì²­í‚¹ ë° ì„ë² ë”© ì²˜ë¦¬

            ì´ì œ search_contentë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
        """
        
    except Exception as e:
        logger.error(f"ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
        return f"âŒ ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


@mcp.tool()
async def search_content(query: str, n_results: int = 10) -> str:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    LlamaIndexì˜ ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    
    Args:
        query: ê²€ìƒ‰í•  ë‚´ìš©
        n_results: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜
    """
    global index
    
    try:
        if index is None:
            # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ
            index = VectorStoreIndex.from_vector_store(
                vector_store,
                storage_context=storage_context
            )
        
        # Retriever ì„¤ì • (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
        retriever = VectorIndexRetriever(
            index=index,
            similarity_top_k=n_results * 2,  # ë” ë§ì€ í›„ë³´êµ° í™•ë³´ í›„ í•„í„°ë§
            vector_store_query_mode="hybrid"
        )
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        nodes = retriever.retrieve(query)
    
        if not nodes:
            return f"'{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        seen_titles = set()
        results = []
        
        for node in nodes:
            title = node.metadata.get('title', 'Untitled')
            
            if title not in seen_titles:
                seen_titles.add(title)
                results.append({
                    'title': title,
                    'platform': node.metadata.get('platform', 'Unknown'),
                    'url': node.metadata.get('url', ''),
                    'date': node.metadata.get('date', ''),
                    'score': node.score,
                    'text': node.text[:200] + "..."
                })
                
                if len(results) >= n_results:
                    break
        
        # ê²°ê³¼ í¬ë§·íŒ…
        output = f"# ğŸ” ê²€ìƒ‰ ê²°ê³¼: '{query}'\n\n"
        output += f"ì´ {len(results)}ê°œì˜ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n\n"
        
        for i, result in enumerate(results, 1):
            output += f"## {i}. [{result['title']}]({result['url']})\n"
            output += f"**í”Œë«í¼**: {result['platform']} | **ë‚ ì§œ**: {result['date']}\n"
            output += f"**ê´€ë ¨ë„**: {result['score']:.3f}\n"
            output += f"**ë¯¸ë¦¬ë³´ê¸°**: {result['text']}\n\n"
        
        return output
        
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


if __name__ == "__main__":
    mcp.run()